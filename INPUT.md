### Small mathematical research to master programming

#### Activation functions

In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard integrated circuit can be seen as a digital network of activation functions that can be "ON" (1) or "OFF" (0), depending on input. This is similar to the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities. Activation function.ipynb script will plot the most known activation functions (ReLu, Sigmoid, Tangens, Step, Arctan, Gaussian) and show how they work in Python. For some activation function, two different inputs will be used. 

#### Monte Carlo simulation of the number pi

The simulation works on the principle of generating random points in the square of the page r = 1.
Based on the distance of the point of the center of the coordinate system, it is possible to determine whether the point is inside or outside imaginary quarters of a circle. How is a quarter of the surface of a circle
<img src="https://render.githubusercontent.com/render/math?math=\frac{r^2\pi}{4}>, and area of square 
$$r^2$$, number $\pi$ can be approximated as 
$$\frac{k}{m} * 4 $$ 
where k is the number of points within a quarter of a circle and m is the total number of simulated random points. As the number of randomly generated points increases, it is possible to calculate the number 
$$\pi$$ 
with greater precision at the expense of time efficiency.

#### Chaos formula and ploting convergence
#### Function for calculating integrals with different levels of precision
#### Optimizing the brute force algorithm to find the shortest path among randomly created points
#### Monty Hall's paradox
#### Birthday problem k people in the same room
#### Prime number generation in parallel
#### Bootstrapping function
#### Area of intesected circles
